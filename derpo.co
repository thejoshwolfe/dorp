
token_patterns =
  * name: \whitespace, regex: /\s+/
  * name: \identifier, regex: /[A-Za-z_][A-Za-z_0-9]*/
  * name: \number,     regex: /[0-9]+/
  * name: \symbol,     regex: /[;+\-*\/=]/

function tokenize source, patterns
  patterns = patterns.concat [{name: null, regex: /./}]
  regex = new RegExp ("(#{pattern.regex.source})" for pattern of patterns).join("|")
  tokens = []
  offset = 0
  while source.length > 0
    match = source.match regex
    type = (patterns[i].name if match[i+1] != null for i from 0 til patterns.length)[0]
    if type == null
      throw new Error "invalid character #{JSON.stringify source[0]} at offset #{offset}"
    if type is not \whitespace
      tokens.push {text: match[0], offset, type: type}
    source = source.substr match.index + match[0].length
    offset += match[0].length
  tokens

parse_rules = do ->
  # rules
  return {}=
    block: list_with_optional_elements \expression, \;, type: \Block
    expression: matcher: any rule(\definition), rule(\assignment)
    definition:
      matcher: sequence token(\identifier, \def), rule(\assignment)
      type: \Definition
      transform: !(node) -> node.value = node.value.1
    assignment: operator_chain \sum, <[=]>, type: \Assignment
    sum: operator_chain \term, <[+ -]>, type: \Arithmetic
    term: operator_chain \atom, <[* /]>, type: \Multiplicative
    atom: matcher: any rule(\number), rule(\identifier)
    number:
      matcher: token \number
      type: \Number
      transform: !(node) -> node.value = Number node.value.text
    identifier:
      matcher: token \identifier
      type: \Identifier
      transform: !(node) -> node.value = node.value.text

  # strutures to make the rules
  function token token_type, text then {control: \token, token_type, text}
  function rule name              then {control: \rule,  name}
  function sequence               then {control: \sequence, matchers: Array.prototype.slice.apply arguments}
  function maybe matcher          then {control: \maybe, matcher}
  function repeat matcher         then {control: \repeat, matcher}
  function any                    then {control: \any, matchers: Array.prototype.slice.apply arguments}

  # utility functions for generating rules
  function list_with_optional_elements rule_name, separator, options
    matcher = maybe sequence rule(rule_name), repeat sequence token(\symbol, separator), maybe rule(rule_name)
    transform = !(node) ->
      return node.value = [] unless node.value?
      result = [node.value.0]
      rest = node.value.1.value
      for separator_and_element of rest
        element = separator_and_element.value.1
        result.push element if element.value?
      node.value = result
    return {options.type, matcher, transform}
  function operator_chain rule_name, operators, options
    matcher = sequence rule(rule_name), repeat sequence any(...(token(\symbol, operator) for operator of operators)), rule(rule_name)
    transform = !(node) ->
      left = node.value.0
      for operator_and_right of node.value.1.value
        [operator, right] = operator_and_right.value
        operator.value = operator.value.text
        left = {
          left.token_start
          right.token_end
          options.type
          value: [left, operator, right]
        }
      node <<< {left.value, left.type}
    return {matcher, transform}

function parse tokens, rules, top_level_type
  root = parse_node top_level_type, 0
  if root.token_end < tokens.length
    throw new Error "expected end of input at offset #{tokens[root.token_end].offset}"
  return root
  function parse_node type, index, null_on_error
    rule = rules[type]
    result = deal_with_matcher rule.matcher, index, null_on_error
    if result?
      rule.transform result if rule.transform?
      result <<< {rule.type} if rule.type?
    return result
    function deal_with_matcher matcher, index, null_on_error
      throw_errors = if null_on_error then null else true
      result = token_start: index
      switch matcher.control
        case \token
          token = tokens[index]
          unless token?
            return throw_errors and throw new Error "expected #{matcher.token_type}, got end of input"
          if token.type is not matcher.token_type
            return throw_errors and throw new Error "expected #{matcher.token_type}, got #{token.type} at offset #{token.offset}"
          if matcher.text? and token.text is not matcher.text
            return throw_errors and throw new Error "expected #{JSON.stringify matcher.text}, got #{JSON.stringify token.text} at offset #{token.offset}"
          result.value = token
          result.token_end = index + 1
        case \rule
          result = parse_node matcher.name, index, null_on_error
        case \sequence
          result.value = []
          for sub_matcher of matcher.matchers
            sub_result = deal_with_matcher sub_matcher, index, null_on_error
            return null unless sub_result?
            result.value.push sub_result
            index = sub_result.token_end
            null_on_error = false
          result.token_end = index
        case \maybe
          maybe_result = deal_with_matcher matcher.matcher, index, true
          if maybe_result?
            result = maybe_result
          else
            result.token_end = index
        case \repeat
          result.value = []
          while true
            sub_result = deal_with_matcher matcher.matcher, index, true
            break unless sub_result?
            result.value.push sub_result
            index = sub_result.token_end
          result.token_end = index
        case \any
          for sub_matcher of matcher.matchers
            sub_result = deal_with_matcher sub_matcher, index, true
            break if sub_result?
          unless sub_result?
            return throw_errors and throw new Error "expected things, got none of them at offset #{tokens[index].offset}"
          result = sub_result
        default throw new Error matcher.control
      if result?
        matcher.transform result if matcher.transform?
        result <<< {matcher.type} if matcher.type?
      result

function evaluate node, context
  switch node.type
    case \Block
      passes = [[],[]]
      for sub_node of node.value
        if sub_node.type is \Definition
          passes.0.push sub_node
        else
          passes.1.push sub_node
      for pass of passes
        for sub_node of pass
          value = evaluate sub_node, context
      value
    case \Definition
      if node.value.type is not \Assignment
        throw new Error "#{JSON.stringify \def} must be followed by an assignment"
      evaluate node.value, context
    case \Assignment
      value = evaluate node.value.2, context
      target = node.value.0
      if target.type is not \Identifier
        throw new Error "can only assign to identifiers. at token index #{target.token_start}"
      context[target.value] = value
    case \Arithmetic, \Multiplicative
      left = evaluate node.value.0, context
      right = evaluate node.value.2, context
      switch node.value.1.value
        case \+ then left + right
        case \- then left - right
        case \* then left * right
        case \/ then left / right
        default throw new Error
    case \Number
      node.value
    case \Identifier
      value = context[node.value]
      unless value?
        throw new Error "undefined variable #{JSON.stringify node.value}. at token index #{node.token_start}"
      value
    default throw new Error node.type

exports.evaluate = (source) ->
  tokens = tokenize source, token_patterns
  program = parse tokens, parse_rules, \block
  # console.log JSON.stringify program, null, "  "
  evaluate program, {}
