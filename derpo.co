
token_patterns =
  * name: \whitespace, regex: /\s+/
  * name: \identifier, regex: /[A-Za-z_][A-Za-z_0-9]*/
  * name: \number,     regex: /[0-9]+/
  * name: \symbol,     regex: /[;]/

function tokenize source, patterns
  patterns = patterns.concat [{name: null, regex: /./}]
  regex = new RegExp ("(#{pattern.regex.source})" for pattern of patterns).join("|")
  tokens = []
  offset = 0
  while source.length > 0
    match = source.match regex
    type = (patterns[i].name if match[i+1] != null for i from 1 til patterns.length)[0]
    if type == null
      throw new Error "invalid character #{JSON.stringify source[0]} at offset #{offset}"
    tokens.push {text: match[0], offset, type: type}
    source = source.substr match.index + match[0].length
    offset += match[0].length
  tokens

parse_rules = do ->
  number = token \number
  number.type = \Number
  number.transform = (token) -> parseInt token.text, 10
  return {}=
    block:
      type: \Block
      matcher: sequence rule(\expression), token(\symbol, \;), rule(\expression)
      # discard separators
      transform: (elements) -> (element if i % 2 == 0 for element, i of elements)
    expression:
      matcher: number
  function token token_type, text then {control: \token, token_type, text}
  function rule name        then {control: \rule,  name}
  function sequence         then {control: \sequence, matchers: Array.prototype.slice.apply arguments}

function parse tokens, rules, top_level_type
  root = parse_node top_level_type, 0
  if root.token_end < tokens.length
    throw new Error "expected end of input at offset #{tokens[root.token_end].offset}"
  return root
  function parse_node type, index
    rule = rules[type]
    result = deal_with_matcher rule.matcher, index
    node = token_start: index
    node <<< {result.token_end, result.value, result.type}
    if rule.transform?
      node.value = rule.transform node.value
    node <<< {rule.type} if rule.type?
    return node
    function deal_with_matcher matcher, index
      token = tokens[index]
      result = token_start: index
      switch matcher.control
        case \token
          if token.type is not matcher.token_type
            throw new Error "expected #{matcher.token_type}, got #{token.type} at offset #{token.offset}"
          if matcher.text? and token.text is not matcher.text
            throw new Error "expected #{JSON.stringify matcher.text}, got #{JSON.stringify token.text} at offset #{token.offset}"
          result.value = token
          result.token_end = index + 1
        case \rule
          result = parse_node matcher.name, index
        case \sequence
          result.value = []
          for sub_matcher of matcher.matchers
            sub_result = deal_with_matcher sub_matcher, index
            result.value.push sub_result
            index = sub_result.token_end
          result.token_end = index
        default throw new Error
      if matcher.transform?
        result.value = matcher.transform result.value
      result <<< {matcher.type} if matcher.type?
      result

exports.evaluate = evaluate
function evaluate source
  tokens = tokenize source, token_patterns
  program = parse tokens, parse_rules, \block
  console.log JSON.stringify program, null, "  "
